import glob
import os
import numpy
import shutil
import pandas as pd
import random
import numpy as np
import multiprocessing
from multiprocessing import Process
import codecs
import random as r
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import log_loss
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier

def normalize(df):
    result1 = df.copy()
    for feature_name in df.columns:
        if (str(feature_name) != str('filename') and str(feature_name)!=str('family')):
            max_value = df[feature_name].max()
            min_value = df[feature_name].min()
            result1[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)
    return result1

def plot_confusion_matrix(y_test, y_test_predict):
    C = confusion_matrix(y_test, y_test_predict)
    print("Number of misclassified points ", (len(y_test) - np.trace(C)) / len(y_test) * 100)
    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j

    A = (((C.T) / (C.sum(axis=1))).T)
    # divid each element of the confusion matrix with the sum of elements in that column

    # C = [[1, 2],
    #     [3, 4]]
    # C.T = [[1, 3],
    #        [2, 4]]
    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array
    # C.sum(axix =1) = [[3, 7]]
    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]
    #                           [2/3, 4/7]]

    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]
    #                           [3/7, 4/7]]
    # sum of row elements = 1

    B = (C / C.sum(axis=0))
    # divid each element of the confusion matrix with the sum of elements in that row
    # C = [[1, 2],
    #     [3, 4]]
    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array
    # C.sum(axix =0) = [[4, 6]]
    # (C/C.sum(axis=0)) = [[1/4, 2/6],
    #                      [3/4, 4/6]]

    # 混淆矩阵
    labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    cmap = sns.light_palette("green")
    # representing A in heatmap format
    print("-" * 50, "Confusion matrix", "-" * 50)
    plt.figure(figsize=(10, 5))
    sns.heatmap(C, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.show()

    # 查准率：Precision = TP/(TP+FP)
    print("-" * 50, "Precision matrix", "-" * 50)
    plt.figure(figsize=(10, 5))
    sns.heatmap(B, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.show()
    print("Sum of columns in precision matrix", B.sum(axis=0))

    # 召回概率：Recall = TP/(TP+FN)
    # representing B in heatmap format
    print("-" * 50, "Recall matrix", "-" * 50)
    plt.figure(figsize=(10, 5))
    sns.heatmap(A, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.show()
    print("Sum of rows in precision matrix", A.sum(axis=1))


if __name__ == "__main__":
    os.chdir(r"D:\research\datasets\3_malware\CCF_1_train_set\asm\output")
    df = pd.read_csv("output.csv")
    result_asm = df.iloc[:, 1:]  # 去掉未命名列
    result_asm = normalize(result_asm)  # 标准化
    asm_y = result_asm['family']
    asm_x = result_asm.drop(['filename', 'family', 'rtn'], axis=1)
    X_train_asm, X_test_asm, y_train_asm, y_test_asm = train_test_split(asm_x, asm_y, stratify=asm_y, test_size=0.20, random_state=41) # stratify:y 按照标签划分
    X_train_asm, X_cv_asm, y_train_asm, y_cv_asm = train_test_split(X_train_asm, y_train_asm, stratify=y_train_asm,
                                                                    test_size=0.20, random_state=41)


    x_cfl = XGBClassifier()
    prams = {
        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2],
        'n_estimators': [100, 200, 500, 1000, 2000],
        'max_depth': [3, 5, 10],
        'colsample_bytree': [0.1, 0.3, 0.5, 1], # 剪枝参数：控制生成树时，随机抽样的比例
        'subsample': [0.1, 0.3, 0.5, 1]
    }
    random_cfl = RandomizedSearchCV(
        x_cfl,
        param_distributions=prams,
        verbose=10,
        n_jobs=-1
    )
    random_cfl.fit()






