import os
import torch
from torch.utils.data import DataLoader
import sys
import torch
import torchvision
from torch import nn, optim
from torchvision import transforms as T
import scipy.io as sio
import h5py
import numpy as np
import matplotlib.pyplot as plt
import random
from time import time
# pretrained VGG
# from model.vgg16_1 import MalVgg16
from model.vgg16_2 import MalVgg16
from config.vgg16_config import Configure
from utils.MalimgDataset import MalimgDataset
from config.EarlyStopping import EarlyStopping
from sklearn.model_selection import StratifiedKFold

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class Configure:
    # 10 fold cross validation
    # base_path = r"../../autodl-tmp/malimg_paper_dataset_imgs"
    base_path = r"D:\research\datasets\3_malware\CCF_1_train_set\train_images"
    malimg_path = r"D:\research\datasets\3_malware\malimg_paper_dataset_imgs"
    model_path = "out/checkpoint/vgg16.pth"  # 保存模型的路径
    load_model = False  # 是否要加载模型
    batch_size = 1
    epochs = 25
    lr = 0.001
    decay = 0.0005
    momentum = 0.9

    gamma = 0
    wd = 0
    tol =  10 ** (-5)
    alpha = 0.99

# 一一个batch的迭代函数
def IterOnce(net, criterion, opt, x, y):
    """
    对模型进行一个batch的迭代

    net: 实例化后的架构
    criterion: 损失函数
    opt: 优化算法
    x: 这一个batch中所有的样本
    y: 这一个batch中所有样本的真实标签
    """
    sigma = net.forward(x)
    loss = criterion(sigma, y)
    loss.backward()
    opt.step()
    opt.zero_grad(set_to_none=True)  # 比起设置梯度为0，让梯度为None会更节约内存
    y_predict = torch.max(sigma, 1)[1]
    correct = torch.sum(y_predict == y)
    return correct, loss
def TestOnce(net, criterion, x, y):
    """
    对一组数据进行测试并输出测试结果的函数

    net: 经过训练后的架构
    criterion：损失函数
    x：要测试的数据的所有样本
    y：要测试的数据的真实标签
    """
    # 对测试，一定要阻止计算图追踪
    # 这样可以节省很多内存，加速运算
    with torch.no_grad():
        sigma = net.forward(x)
        loss = criterion(sigma, y)
        yhat = torch.max(sigma, 1)[1]
        correct = torch.sum(yhat == y)
    return correct, loss

def fit_test(net, train_data, test_data, criterion, opt, epochs, tol, modelname, PATH):
    """
    对模型进行训练，并在每个epoch后输出训练集和测试集上的准确率/损失
    以实现对模型的监控
    实现模型的保存

    参数说明：
    net: 实例化后的网络
    train_data：使用Dataloader分割后的训练数据
    test_data：使用Dataloader分割后的测试数据
    criterion：所使用的损失函数
    opt：所使用的优化算法
    epochs：一共要使用完整数据集epochs次
    tol：提前停止时测试集上loss下降的阈值，连续5次loss下降不超过tol就会触发提前停止
    modelname：现在正在运行的模型名称，用于保存权重时作为文件名
    PATH：将权重文件保存在path目录下

    """

    SamplePerEpoch = train_data.dataset.__len__()  # 整个epoch里有多少个样本
    allsamples = SamplePerEpoch * epochs
    trainedsamples = 0
    trainlosslist = []
    testlosslist = []
    early_stopping = EarlyStopping(tol=tol)
    highestacc = None

    for epoch in range(1, epochs + 1):
        net.train()
        correct_train = 0
        loss_train = 0
        for batch_idx, (x, y) in enumerate(train_data):
            y = y.view(x.shape[0])
            correct, loss = IterOnce(net, criterion, opt, x, y)
            trainedsamples += x.shape[0]
            loss_train += loss
            correct_train += correct

            if (batch_idx + 1) % 125 == 0:
                # 现在进行到了哪个epoch
                # 现在训练到了多少个样本
                # 总共要训练多少个样本
                # 现在的训练的样本占总共需要训练的样本的百分比
                print('Epoch{}:[{}/{}({:.0f}%)]'.format(epoch
                                                        , trainedsamples
                                                        , allsamples
                                                        , 100 * trainedsamples / allsamples))

        TrainAccThisEpoch = float(correct_train * 100) / SamplePerEpoch
        TrainLossThisEpoch = float(loss_train * 100) / SamplePerEpoch  # 平均每个样本上的损失
        trainlosslist.append(TrainLossThisEpoch)

        # 每次训练完一个epoch，就在测试集上验证一下模型现在的效果
        net.eval()
        loss_test = 0
        correct_test = 0
        loss_test = 0
        TestSample = test_data.dataset.__len__()

        for x, y in test_data:
            y = y.view(x.shape[0])
            correct, loss = TestOnce(net, criterion, x, y)
            loss_test += loss
            correct_test += correct

        TestAccThisEpoch = float(correct_test * 100) / TestSample
        TestLossThisEpoch = float(loss_test * 100) / TestSample
        testlosslist.append(TestLossThisEpoch)

        # 对每一个epoch，打印训练和测试的结果
        # 训练集上的损失，测试集上的损失，训练集上的准确率，测试集上的准确率
        print("\t Train Loss:{:.6f}, Test Loss:{:.6f}, Train Acc:{:.3f}%, Test Acc:{:.3f}%".format(TrainLossThisEpoch
                                                                                                   , TestLossThisEpoch
                                                                                                   , TrainAccThisEpoch
                                                                                                   , TestAccThisEpoch))

        # 如果测试集准确率出现新高/测试集loss出现新低，那我会保存现在的这一组权重
        if highestacc == None:  # 首次进行测试
            highestacc = TestAccThisEpoch
        if highestacc < TestAccThisEpoch:
            highestacc = TestAccThisEpoch
            torch.save(net.state_dict(), os.path.join(PATH, modelname + ".pt"))
            print("\t Weight Saved")

        # 提前停止
        early_stop = early_stopping(TestLossThisEpoch)
        if early_stop == "True":
            break

    print("Complete")
    return trainlosslist, testlosslist


def full_procedure(net, train_set, test_set, epochs, bs, modelname):
    torch.manual_seed(1412)
    epochs = Configure.epochs
    batch_size = Configure.batch_size
    lr = Configure.lr
    momentum = Configure.momentum

    # 分割数据
    train_data = DataLoader(train_set, batch_size=bs, shuffle=True
                           , drop_last=False, num_workers=4)  # 线程 - 调度计算资源的最小单位
    test_data = DataLoader(test_set, batch_size=bs, shuffle=False
                          , drop_last=False, num_workers=4)

    # 损失函数，优化算法
    criterion = nn.CrossEntropyLoss(reduction="sum")  # 进行损失函数计算时，最后输出结果的计算模式
    opt = optim.RMSprop(net.parameters(), lr=lr
                        , alpha=Configure.alpha, momentum=Configure.gamma, weight_decay=Configure.wd)

    # 训练与测试
    PATH = Configure.model_path
    trainloss, testloss = fit_test(net, train_data, test_data, criterion, opt, epochs, Configure.tol, modelname, PATH)

    return trainloss, testloss

#绘图函数
def plotloss(trainloss, testloss):
    plt.figure(figsize=(10, 7))
    plt.plot(trainloss, color="red", label="Trainloss")
    plt.plot(testloss, color="orange", label="Testloss")
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


def load_model(target_model, model_path):
    # 加载预模型
    if not os.path.exists(model_path):
        print("load model error")
        sys.exit(0)
    else:
        target_model.load_state_dict(torch.load(model_path))
        target_model.eval()


# def train(model, epoch, train_loader, optimizer):
#     for batch_idx, data in enumerate(train_loader, 0):
#         optimizer.zero_grad()  # 梯度清0
#
#         inputs, labels = data
#         inputs, labels = inputs.to(device), labels.to(device)
#
#         y_pred = model(inputs)  # 前向传播
#         loss = torch.nn.functional.cross_entropy(y_pred, labels.long())  # 计算损失
#
#         if batch_idx % 100 == 99 or batch_idx % 5 == 0:
#             print("epoch=%d, loss=%f" % (epoch, loss.item()))
#
#         loss.backward()  # 反向传播
#         optimizer.step()  # 梯度更新

if __name__ == "__main__":
    config = Configure()
    train_set = MalimgDataset(config.base_path)
    train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True, num_workers=2)

    if Configure.load_model:
        model = MalVgg16()
        print("----load model-----")
        load_model(model, config.model_path)
    else:
        model = MalVgg16() # pretrained=True
    model.to(device)

    # 未导入外部模型
    if not Configure.load_model:
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr = config.lr,
            weight_decay=config.decay,
            momentum=config.momentum
        )
    print("-----training begin-----")
    # for i in range(config.epochs):
    #     train(model, i, train_loader, optimizer)



